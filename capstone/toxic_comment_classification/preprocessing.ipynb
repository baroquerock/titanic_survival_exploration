{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import itertools\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.translate import NotTranslated\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "\n",
    "np.random.seed(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = r'[\\d]+(?:\\.[\\d]+){3}'\n",
    "url = r'http\\:[^\\s]+'\n",
    "digit = r'[\\d]+'\n",
    "user = r'[Uu]ser\\:[^\\s]+'\n",
    "\n",
    "def clean_text(text): \n",
    "    text = re.sub(ip,'',text)\n",
    "    text = re.sub(url,'',text)  \n",
    "    text = re.sub(digit,'',text)\n",
    "    text = re.sub(user,'',text)\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))       \n",
    "    tokens = word_tokenize(text)   \n",
    "    tokens = [x for x in tokens if len(x) > 1]\n",
    "    joined = str(' '.join(tokens))\n",
    "    return joined\n",
    "\n",
    "# translating text into target language and back to english (google API)\n",
    "def translate(comment, language):\n",
    "    try:\n",
    "        if hasattr(comment, \"decode\"):\n",
    "            comment = comment.decode(\"utf-8\")\n",
    "        text = TextBlob(comment)\n",
    "        text = text.translate(to=language)\n",
    "        text = text.translate(to=\"en\")\n",
    "        return str(text)\n",
    "    except Exception:\n",
    "        return None \n",
    "    \n",
    "# alternative way to cut sentences with length more than X:\n",
    "# take the first X/2 words and the last X/2 words\n",
    "def cut_to_x(text, x):\n",
    "    words = text.split()\n",
    "    if len(words) > x:\n",
    "        words = words[:100] + words[-100:]\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# complete text preprocessing according to the predefined params: max_features, maxlen and the exact text column\n",
    "# this function (1) adds additional column to the train and test dataframes which represents preprocessed text \n",
    "# and (2) creates a pickle with tokenizer used in preprocessing\n",
    "def preprocess_text(max_features, maxlen, train, test, on='tokenized_text', postfix=''):   \n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(pd.concat([train[on], test[on]]))\n",
    "    \n",
    "    tokenized_train = tokenizer.texts_to_sequences(train[on])\n",
    "    tokenized_test = tokenizer.texts_to_sequences(test[on])\n",
    "    \n",
    "    padded_train = pad_sequences(tokenized_train, maxlen=maxlen)\n",
    "    padded_test = pad_sequences(tokenized_test, maxlen=maxlen)\n",
    "    \n",
    "    cname = 'padded_{0}_{1}_{2}'.format(maxlen, max_features, postfix)\n",
    "    \n",
    "    print('New column name: {}'.format(cname))\n",
    "    \n",
    "    train[cname] = list(padded_train)\n",
    "    test[cname] = list(padded_test)\n",
    "    \n",
    "    with open('pickles/tokenizer_{0}_{1}_{2}.pickle'.format(maxlen, max_features, postfix), 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path + 'train.csv')\n",
    "test = pd.read_csv(data_path + 'test.csv')\n",
    "train.replace(np.nan, '', regex=True, inplace=True)\n",
    "test.replace(np.nan, '', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(train)) < 0.85\n",
    "train['split'] = msk\n",
    "train['lang'] = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Augmentation through translation (with Google API)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences for translation: 2542\n",
      "Translate comments using de language\n",
      "Translate comments using fr language\n",
      "Translate comments using es language\n"
     ]
    }
   ],
   "source": [
    "for_translation = train[(train['severe_toxic'] > 0) | (train['threat'] > 0) | (train['identity_hate'] > 0)][(train['split'] == True)]\n",
    "print('Sentences for translation: {}'.format(len(for_translation)))\n",
    "\n",
    "to_use = ['id','toxic','severe_toxic','obscene','threat','insult','identity_hate','split']\n",
    "\n",
    "parallel = Parallel(200, backend='threading', verbose=0)\n",
    "\n",
    "for lang in ['de', 'fr', 'es']:\n",
    "    print('Translate comments using {0} language'.format(lang))\n",
    "    translated_data = parallel(delayed(translate)(comment, lang) for comment in for_translation['comment_text'].values)\n",
    "    translated_df = for_translation[to_use]\n",
    "    translated_df['comment_text'] = translated_data\n",
    "    translated_df['lang'] = lang\n",
    "    translated_df = translated_df[pd.notnull(translated_df['comment_text'])]\n",
    "    frames = [train, translated_df]\n",
    "    train = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning and tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokenized_text_upper'] = train['comment_text'].astype(str).apply(clean_text)\n",
    "test['tokenized_text_upper'] = test['comment_text'].astype(str).apply(clean_text)\n",
    "\n",
    "train['tokenized_text_lower'] = train['tokenized_text_upper'].astype(str).apply(lambda x: x.lower())\n",
    "test['tokenized_text_lower'] = test['tokenized_text_upper'].astype(str).apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokenized_text_lower_cut'] = train['tokenized_text_lower'].astype(str).apply(lambda x: cut_to_x(x, 200))\n",
    "test['tokenized_text_lower_cut'] = test['tokenized_text_lower'].astype(str).apply(lambda x: cut_to_x(x, 200))\n",
    "\n",
    "train['tokenized_text_upper_cut'] = train['tokenized_text_upper'].astype(str).apply(lambda x: cut_to_x(x, 200))\n",
    "test['tokenized_text_upper_cut'] = test['tokenized_text_upper'].astype(str).apply(lambda x: cut_to_x(x, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experimenting with max_features/maxlen parameters on augmented dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to be explored\n",
    "params = {}\n",
    "params['maxlen'] = [200] #[150,200,250,300]\n",
    "params['max_features'] = [50000] #[30000,40000,50000,60000]\n",
    "params['is_lower'] = [True] #[True, False]\n",
    "params['is_cut'] = [False] #[True, False]\n",
    "\n",
    "names = ['maxlen','max_features', 'is_lower', 'is_cut']\n",
    "combinations = itertools.product(*(params[name] for name in names))\n",
    "combinations = list(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: (200, 50000, True, False)\n",
      "New column name: padded_200_50000_lower\n"
     ]
    }
   ],
   "source": [
    "for i,comb in enumerate(combinations):\n",
    "    print('Iteration: {}'.format(comb))\n",
    "    maxlen = comb[0]\n",
    "    max_features = comb[1]\n",
    "    is_lower = comb[2]\n",
    "    is_cut = comb[3]\n",
    "    if is_lower and is_cut:\n",
    "        preprocess_text(max_features, maxlen, train, test, on='tokenized_text_lower_cut', postfix='lower_cut')\n",
    "    elif is_lower and not is_cut:\n",
    "        preprocess_text(max_features, maxlen, train, test, on='tokenized_text_lower', postfix='lower')\n",
    "    elif not is_lower and is_cut:\n",
    "        preprocess_text(max_features, maxlen, train, test, on='tokenized_text_upper_cut', postfix='upper_cut')\n",
    "    elif not is_lower and not is_cut:\n",
    "        preprocess_text(max_features, maxlen, train, test, on='tokenized_text_upper', postfix='upper')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(data_path + 'train_pre.csv',index=False)\n",
    "test.to_csv(data_path + 'test_pre.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
