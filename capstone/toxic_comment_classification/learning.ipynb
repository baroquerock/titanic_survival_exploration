{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.layers import Dense, GRU, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seed to take care of reproducibility issues (unfortunately only partial solution)\n",
    "# ref: https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "def set_seed(s):\n",
    "    np.random.seed(s)\n",
    "    rn.seed(s)\n",
    "    tf.set_random_seed(s)\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# helper function to get the name of the train column in the preprocessed dataset \n",
    "# ref: preprocessing.ipynb\n",
    "def get_col_name(maxlen=150, max_features=30000, cut=False, lower=False):\n",
    "    if lower:\n",
    "        if cut:\n",
    "            postfix = 'lower_cut'\n",
    "        else:\n",
    "            postfix = 'lower'\n",
    "    else:\n",
    "        if cut:\n",
    "            postfix = 'upper_cut'\n",
    "        else:\n",
    "            postfix = 'upper'\n",
    "    \n",
    "    name = '{}_{}_{}'.format(maxlen, max_features, postfix)\n",
    "    return name\n",
    "\n",
    "# helper function to read word embeddings\n",
    "def get_entry(word,*arr): \n",
    "    try:\n",
    "        array = np.asarray(arr, dtype='float32')\n",
    "        return word, array\n",
    "    except Exception:\n",
    "        return 'None', 'None'\n",
    "\n",
    "# helper function to restore preprocessed values\n",
    "def parse_to_ints(line): return list(map(int, line[1:-1].split())) \n",
    "\n",
    "# reading embeddings from a file and transforming them into dict - {word:vector}\n",
    "def get_embeddings_index(filename):\n",
    "    index = dict(get_entry(*o.strip().split()) for o in open(embeddings_path+filename))\n",
    "    embeddings_index = {k:v for k,v in index.items() if len(v) == dim}\n",
    "    return embeddings_index\n",
    "\n",
    "# transforming embedding index into embedding matrix with shape (max_features, dim) \n",
    "# that will serve as an initialization to the Embedding layer of LSTM or GRU neural network \n",
    "# dim is a globally defined variable\n",
    "def get_embedding_matrix(embeddings_index, tokenizer):\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector    \n",
    "    return embedding_matrix\n",
    "\n",
    "# compiling LSTM model\n",
    "# dim is a globally defined variable\n",
    "def get_model_lstm(embedding_matrix, n=200, dropout=0.2, recurrent_dropout=0.1, pool='global_max'):\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, dim, weights=[embedding_matrix])(inp)\n",
    "    x = Bidirectional(LSTM(n, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))(x)\n",
    "    #x = LSTM(n, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout)(x)\n",
    "    \n",
    "    if pool == 'global_max':\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "    elif pool == 'global_average':\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        \n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# compiling GRU model\n",
    "# dim is a globally defined variable\n",
    "def get_model_gru(embedding_matrix, n=200, dropout=0.2, recurrent_dropout=0.1, pool='global_max'):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, dim, weights=[embedding_matrix])(inp)\n",
    "    x = Bidirectional(GRU(n, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))(x)\n",
    "    #x = GRU(n, return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout)(x)\n",
    "    \n",
    "    if pool == 'global_max':\n",
    "        x = GlobalMaxPool1D()(x)\n",
    "    elif pool == 'global_average':\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        \n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# helper function to get performance evaluation\n",
    "# result_df is a DataFrame with predefined columns\n",
    "# prefix is a list of any values that should be written before performance stats:\n",
    "# example: prefix = [n,recurrent_dropout,dropout,pooling]\n",
    "def print_performance_stats(y_test, y_pred, result_df, prefix, verbose=0):\n",
    "    \n",
    "    scores = []\n",
    "    for c in list_classes:\n",
    "        precision = precision_score(y_test[c], y_pred[c].round())\n",
    "        recall = recall_score(y_test[c], y_pred[c].round())\n",
    "        f1 = f1_score(y_test[c], y_pred[c].round())\n",
    "        score = roc_auc_score(y_test[c], y_pred[c])\n",
    "        scores.append(score)\n",
    "        \n",
    "        prefix += [score, precision, recall, f1]\n",
    "    \n",
    "        if verbose:\n",
    "            print('roc_auc_score score on {0}: {1}'.format(c, score))\n",
    "            print('Precision/Recall on {0}: {1}/{2}'.format(c, precision,recall))\n",
    "            print('F1 score on {0}: {1}'.format(c, f1))\n",
    "            print('')\n",
    "    \n",
    "    final_score = sum(scores)/6\n",
    "    prefix.append(final_score)\n",
    "    \n",
    "    res = {k:v for k,v in zip(columns, prefix)}\n",
    "    result_df = result_df.append(res, ignore_index=True)\n",
    "    \n",
    "    print('Final roc_auc_score score: {0}'.format(final_score))\n",
    "    \n",
    "    return result_df\n",
    "    \n",
    "# helper function to get train-test split based on (1) predefined in preprocessing stage mask\n",
    "# and parameters like augmentation, lowercase etc.\n",
    "def get_train_test(augmented, cname):\n",
    "    \n",
    "    train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "    if not augmented:\n",
    "        train = train[train['lang'] == 'en']\n",
    "    \n",
    "    print('Size of the dataset: {}'.format(len(train)))\n",
    "\n",
    "    train[cname] = train[cname].apply(parse_to_ints)\n",
    "\n",
    "    X_train = np.array(list(train[cname][train['split']]))\n",
    "    X_test = np.array(list(train[cname][~train['split']]))\n",
    "\n",
    "    y = train[list_classes].values\n",
    "    y_train = y[train['split']]\n",
    "    y_test = train[list_classes][~train['split']]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BASIC SETUP** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed data file with transformed and padded text\n",
    "TRAIN_DATA_FILE = 'data/train_pre.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec - https://code.google.com/archive/p/word2vec/\n",
    "#glove - https://nlp.stanford.edu/projects/glove/\n",
    "#fastText - https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "model_path = 'models/'\n",
    "embeddings_path = 'emdeddings/'\n",
    "google_news_300_w2v = 'GoogleNews-vectors-negative300.bin'\n",
    "wiki_300_ft = 'wiki.en.vec'\n",
    "twitter_100_glove = 'glove.twitter.27B.100d.txt'\n",
    "twitter_200_glove = 'glove.twitter.27B.200d.txt'\n",
    "wiki_100_glove = 'glove.6B.100d.txt'\n",
    "wiki_200_glove = 'glove.6B.200d.txt'\n",
    "wiki_300_glove = 'glove.6B.300d.txt'\n",
    "crawl_300_glove = 'glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_columns = ['toxic_auc','toxic_precision','toxic_recall','toxic_f1',\n",
    "                 'severe_auc','severe_precision','severe_recall','severe_f1',\n",
    "                 'obscene_auc','obscene_precision','obscene_recall','obscene_f1',\n",
    "                 'threat_auc','threat_precision','threat_recall','threat_f1',\n",
    "                 'insult_auc','insult_precision','insult_recall','insult_f1',\n",
    "                 'identity_auc','identity_precision','identity_recall','identity_f1',\n",
    "                 'final_auc']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RUNNING EXPERIMENTS WITH VARIOUS PREPROCESSING PARAMETERS (MAXLEN, MAX_FEATURES etc)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining word embeddings\n",
    "dim = 100\n",
    "name = wiki_100_glove\n",
    "name_for_print = 'wiki_100_glove'\n",
    "embeddings_index = get_embeddings_index(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['maxlen','max_features','augmented','cut','lower'] + basic_columns\n",
    "results = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to be explored\n",
    "params = {}\n",
    "params['maxlen'] = [200] #[150,200,250,300]\n",
    "params['max_features'] = [50000] #[30000,40000,50000,60000]\n",
    "params['augmented'] = [True] #[True, False]\n",
    "params['is_lower'] = [True] #[True, False]\n",
    "params['is_cut'] = [False] #[True, False]\n",
    "\n",
    "param_names = ['maxlen','max_features','augmented','is_lower','is_cut']\n",
    "combinations = itertools.product(*(params[name] for name in param_names))\n",
    "combinations = list(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running experiment 1/1\n",
      "Combination (200, 50000, True, True, False)\n",
      "\n",
      "Column name: padded_200_50000_lower\n",
      "Size of the dataset: 160914\n",
      "Train on 123165 samples, validate on 13686 samples\n",
      "Epoch 1/2\n",
      "123165/123165 [==============================] - 1215s 10ms/step - loss: 0.0599 - acc: 0.9794 - val_loss: 0.1087 - val_acc: 0.9600\n",
      "Epoch 2/2\n",
      "123165/123165 [==============================] - 1356s 11ms/step - loss: 0.0427 - acc: 0.9837 - val_loss: 0.0984 - val_acc: 0.9622\n",
      "24063/24063 [==============================] - 48s 2ms/step\n",
      "Final roc_auc_score score: 0.9815273291281138\n"
     ]
    }
   ],
   "source": [
    "# before running this cell ensure that for every combination there is a preprocessed column in the TRAIN_DATA_FILE\n",
    "# and specific tokenizer in the pickle folder\n",
    "for i, comb in enumerate(combinations):\n",
    "    \n",
    "    print('\\n\\nRunning experiment {}/{}'.format(i+1, len(combinations)))\n",
    "    print('Combination {}'.format(comb))\n",
    "    print()\n",
    "    \n",
    "    maxlen = comb[0]\n",
    "    max_features = comb[1]\n",
    "    augmented = comb[2]\n",
    "    lower = comb[3]\n",
    "    cut = comb[4]\n",
    "    \n",
    "    name = get_col_name(maxlen, max_features, cut, lower)\n",
    "    cname = 'padded_{}'.format(name)\n",
    "    print('Column name: {}'.format(cname))\n",
    "    TOKENIZER_PICKLE = 'pickles/tokenizer_{}.pickle'.format(name)\n",
    "    \n",
    "    with open(TOKENIZER_PICKLE, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = get_train_test(augmented, cname)\n",
    "        \n",
    "    assert(tokenizer.__dict__['num_words'] == max_features)\n",
    "    assert(X_train.shape[1] == maxlen)\n",
    "    \n",
    "    set_seed(29)\n",
    "    embedding_matrix = get_embedding_matrix(embeddings_index, tokenizer)\n",
    "    model = get_model_lstm(embedding_matrix, n=100, dropout=0.3, recurrent_dropout=0.1, pool='global_max')\n",
    "    model.fit(X_train, y_train, batch_size=64, epochs=n_epochs, validation_split=0.1)\n",
    "    \n",
    "    y_pred = model.predict([X_test], batch_size=1024, verbose=1)\n",
    "    y_pred = pd.DataFrame(y_pred)\n",
    "    y_pred.columns = list_classes\n",
    "    \n",
    "    prefix = [maxlen,max_features,augmented,cut,lower]\n",
    "    \n",
    "    results = print_performance_stats(y_test, y_pred, result_df=results, prefix=prefix, verbose=0)\n",
    "    results.to_csv('results/results_{}.csv'.format(name_for_print), index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RUNNING EXPERIMENTS WITH VARIOUS EMBEDDINGS** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['embeddings'] + basic_columns\n",
    "results = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [(twitter_200_glove, 200)] \n",
    "#[(wiki_100_glove, 100), (wiki_200_glove, 200), (wiki_300_glove, 300), (wiki_300_ft, 300), \n",
    "#(twitter_100_glove, 100), (twitter_200_glove, 200), (crawl_300_glove, 300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name: padded_200_50000_lower\n",
      "Size of the dataset: 160914\n"
     ]
    }
   ],
   "source": [
    "# using the best params defined in the previous step\n",
    "maxlen = 200\n",
    "max_features = 50000\n",
    "augmented = True\n",
    "lower = True\n",
    "cut = False \n",
    "\n",
    "name = get_col_name(maxlen, max_features, cut, lower)\n",
    "cname = 'padded_{}'.format(name)\n",
    "print('Column name: {}'.format(cname))\n",
    "TOKENIZER_PICKLE = 'pickles/tokenizer_{}.pickle'.format(name)\n",
    "X_train, X_test, y_train, y_test = get_train_test(augmented, cname)\n",
    "\n",
    "with open(TOKENIZER_PICKLE, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "    \n",
    "assert(tokenizer.__dict__['num_words'] == max_features)\n",
    "assert(X_train.shape[1] == maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running experiment 1/1\n",
      "Embeddings: ('glove.twitter.27B.200d.txt', 200)\n",
      "\n",
      "Train on 123165 samples, validate on 13686 samples\n",
      "Epoch 1/2\n",
      "123165/123165 [==============================] - 1555s 13ms/step - loss: 0.0561 - acc: 0.9804 - val_loss: 0.1021 - val_acc: 0.9602\n",
      "Epoch 2/2\n",
      "123165/123165 [==============================] - 1526s 12ms/step - loss: 0.0395 - acc: 0.9847 - val_loss: 0.0951 - val_acc: 0.9619\n",
      "24063/24063 [==============================] - 65s 3ms/step\n",
      "Final roc_auc_score score: 0.9861440821621373\n"
     ]
    }
   ],
   "source": [
    "for i, embed in enumerate(embeddings):\n",
    "    \n",
    "    print('\\n\\nRunning experiment {}/{}'.format(i+1, len(embeddings)))\n",
    "    print('Embeddings: {}'.format(embed))\n",
    "    print()\n",
    "            \n",
    "    name = embed[0]\n",
    "    dim = embed[1]\n",
    "    embeddings_index = get_embeddings_index(name)\n",
    "           \n",
    "    set_seed(29)\n",
    "    embedding_matrix = get_embedding_matrix(embeddings_index, tokenizer)\n",
    "    \n",
    "    model = get_model_lstm(embedding_matrix, n=100, dropout=0.3, recurrent_dropout=0.1, pool='global_max')\n",
    "    model.fit(X_train, y_train, batch_size=64, epochs=n_epochs, validation_split=0.1)\n",
    "    \n",
    "    y_pred = model.predict([X_test], batch_size=1024, verbose=1)\n",
    "    y_pred = pd.DataFrame(y_pred)\n",
    "    y_pred.columns = list_classes \n",
    "        \n",
    "    prefix = [name]\n",
    "    \n",
    "    results = print_performance_stats(y_test, y_pred, result_df=results, prefix=prefix, verbose=0)\n",
    "    results.to_csv('results/embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPERIMENTING WITH ARCHITECHTURE** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['n','recurrent_dropout','dropout','pooling'] + basic_columns\n",
    "results = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the best params defined in the previous steps \n",
    "name = twitter_200_glove\n",
    "dim = 200\n",
    "embeddings_index = get_embeddings_index(name)\n",
    "embedding_matrix = get_embedding_matrix(embeddings_index, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['n'] = [200] #[50, 75, 100, 125, 150, 175, 200, 225, 250]\n",
    "params['recurrent_dropout'] = [0.1] #[0.1, 0.2, 0.3, 0.4]\n",
    "params['dropout'] = [0.2]  #[0.1, 0.2, 0.3, 0.4] \n",
    "params['pooling'] = ['global_max'] #['global_max', 'global_average'] \n",
    "\n",
    "param_names = ['n','recurrent_dropout','dropout', 'pooling']\n",
    "combinations = itertools.product(*(params[name] for name in param_names))\n",
    "combinations = list(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running experiment 1/1\n",
      "Combination: (200, 0.1, 0.2, 'global_max')\n",
      "\n",
      "Train on 123165 samples, validate on 13686 samples\n",
      "Epoch 1/2\n",
      "123165/123165 [==============================] - 2502s 20ms/step - loss: 0.0516 - acc: 0.9815 - val_loss: 0.0955 - val_acc: 0.9619\n",
      "Epoch 2/2\n",
      "123165/123165 [==============================] - 2485s 20ms/step - loss: 0.0383 - acc: 0.9851 - val_loss: 0.0860 - val_acc: 0.9660\n",
      "24063/24063 [==============================] - 111s 5ms/step\n",
      "Final roc_auc_score score: 0.9865930692161983\n"
     ]
    }
   ],
   "source": [
    "for i, comb in enumerate(combinations):\n",
    "    \n",
    "    print('\\n\\nRunning experiment {}/{}'.format(i+1, len(combinations)))\n",
    "    print('Combination: {}'.format(comb))\n",
    "    print()\n",
    "    \n",
    "    n = comb[0]\n",
    "    recurrent_dropout = comb[1]\n",
    "    dropout = comb[2]\n",
    "    pool = comb[3]\n",
    "           \n",
    "    set_seed(29)\n",
    "    model = get_model_lstm(embedding_matrix, n=n, dropout=dropout, recurrent_dropout=recurrent_dropout, pool=pool)\n",
    "    model.fit(X_train, y_train, batch_size=64, epochs=n_epochs, validation_split=0.1)\n",
    "    \n",
    "    y_pred = model.predict([X_test], batch_size=1024, verbose=1)\n",
    "    y_pred = pd.DataFrame(y_pred)\n",
    "    y_pred.columns = list_classes \n",
    "    \n",
    "    prefix = [n,recurrent_dropout,dropout,pool]\n",
    "    \n",
    "    results = print_performance_stats(y_test, y_pred, result_df=results, prefix=prefix, verbose=0)\n",
    "    results.to_csv('results/results_arch.csv'.format(name_for_print), index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING AND SAVING THE FINAL MODEL** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = basic_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefining the optimal parameters\n",
    "maxlen = 200\n",
    "max_features = 50000\n",
    "augmented = True\n",
    "lower = True\n",
    "cut = False \n",
    "\n",
    "n = 200\n",
    "recurrent_dropout = 0.1\n",
    "dropout = 0.2\n",
    "pool = 'global_max'\n",
    "\n",
    "embed = twitter_200_glove\n",
    "dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name: padded_200_50000_lower\n",
      "Size of the dataset: 160914\n"
     ]
    }
   ],
   "source": [
    "name = get_col_name(maxlen, max_features, cut, lower)\n",
    "cname = 'padded_{}'.format(name)\n",
    "print('Column name: {}'.format(cname))\n",
    "TOKENIZER_PICKLE = 'pickles/tokenizer_{}.pickle'.format(name)\n",
    "X_train, X_test, y_train, y_test = get_train_test(augmented, cname)\n",
    "\n",
    "with open(TOKENIZER_PICKLE, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "    \n",
    "assert(tokenizer.__dict__['num_words'] == max_features)\n",
    "assert(X_train.shape[1] == maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = dict(get_entry(*o.strip().split()) for o in open(embeddings_path+embed))\n",
    "embeddings_index = {k:v for k,v in index.items() if len(v) == dim}\n",
    "embedding_matrix = get_embedding_matrix(embeddings_index, tokenizer)\n",
    "model = get_model_lstm(embedding_matrix, n=n, dropout=dropout, recurrent_dropout=recurrent_dropout, pool=pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 123165 samples, validate on 13686 samples\n",
      "Epoch 1/2\n",
      "123165/123165 [==============================] - 2627s 21ms/step - loss: 0.0530 - acc: 0.9810 - val_loss: 0.0957 - val_acc: 0.9626\n",
      "Epoch 2/2\n",
      "123165/123165 [==============================] - 2629s 21ms/step - loss: 0.0385 - acc: 0.9850 - val_loss: 0.0911 - val_acc: 0.9638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11706e278>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://parneetk.github.io/blog/neural-networks-in-keras/\n",
    "set_seed(29)\n",
    "#earlystop = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=5, verbose=1, mode='auto')\n",
    "#callbacks_list = [earlystop]\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=2, validation_split=0.1) # callbacks=callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24063/24063 [==============================] - 111s 5ms/step\n",
      "Final roc_auc_score score: 0.9860371756179931\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([X_test], batch_size=1024, verbose=1)\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "y_pred.columns = list_classes \n",
    "results = print_performance_stats(y_test, y_pred, result_df=pd.DataFrame(), prefix=[], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/final_model.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
