{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['toxic_auc','toxic_precision','toxic_recall','toxic_f1','severe_auc',\\\n",
    "           'severe_precision','severe_recall','severe_f1','obscene_auc','obscene_precision',\\\n",
    "           'obscene_recall','obscene_f1','threat_auc','threat_precision','threat_recall','threat_f1',\\\n",
    "           'insult_auc','insult_precision','insult_recall','insult_f1','identity_auc','identity_precision',\\\n",
    "           'identity_recall','identity_f1','final_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predictions(data, on='comment_text', verbose=0):\n",
    "    \n",
    "    results = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    vec = TfidfVectorizer()\n",
    "    X = vec.fit_transform(data[on])\n",
    "    \n",
    "    if verbose:\n",
    "        print(X.shape)\n",
    "\n",
    "    msk = data['split'].values\n",
    "    X_train = X[msk]\n",
    "    X_test = X[~msk]\n",
    "    \n",
    "    scores = []\n",
    "    models = {}\n",
    "    bad = {}\n",
    "    \n",
    "    labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    \n",
    "    baseline_res = []\n",
    "\n",
    "    for label in labels:\n",
    "    \n",
    "        y = data[label]\n",
    "        y_train = y[msk]\n",
    "        y_test = y[~msk]\n",
    "    \n",
    "        model = LogisticRegression()    \n",
    "        model.fit(X_train, y_train)\n",
    "        models[label] = model\n",
    "    \n",
    "        y_pred = model.predict(X_test)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "           \n",
    "        wrong = [x==y for x,y in zip(y_test,y_pred)]\n",
    "        \n",
    "        indices = np.where(np.array(wrong) == False)\n",
    "        bad[label] = indices\n",
    "    \n",
    "        y_pred = model.predict_proba(X_test) \n",
    "        score = roc_auc_score(y_test, y_pred[:,1])\n",
    "        scores.append(score)\n",
    "        \n",
    "        baseline_res += [score, precision, recall, f1]\n",
    "        \n",
    "        if verbose:\n",
    "            print('roc_auc_score score on {0}: {1}'.format(label, score))\n",
    "            print('Precision/Recall on {0}: {1}/{2}'.format(label, precision,recall))\n",
    "            print('F1 score on {0}: {1}'.format(label, f1))\n",
    "            print('')\n",
    "    \n",
    "    final_score = sum(scores)/6\n",
    "    \n",
    "    baseline_res += [final_score]\n",
    "    \n",
    "    baseline_res = {k:v for k,v in zip(columns, baseline_res)}\n",
    "    results = results.append(baseline_res, ignore_index=True)\n",
    "    results.to_csv('results/baseline.csv', index=False)\n",
    "    print('Final roc_auc_score score: {0}'.format(final_score))\n",
    "    return bad, models, vec, baseline_res"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset: 159571\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/train_pre.csv')\n",
    "train.replace(np.nan, '', regex=True, inplace=True)\n",
    "# running the baseline on non-augmented dataset\n",
    "train = train[train['lang'] == 'en']\n",
    "print('Size of the dataset: {}'.format(len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final roc_auc_score score: 0.9761572006464002\n"
     ]
    }
   ],
   "source": [
    "bad, models, vec, res = run_predictions(train, on='tokenized_text_upper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
